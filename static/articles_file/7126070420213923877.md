

本文已参与「新人创作礼」活动，一起开启掘金创作之路。
## 1.普通卷积神经网络具体结构？各层作用

输入层 全连接层 卷积层 池化层 输出层

输入层：与传统神经网络/机器学习一样，模型需要输入的进行预处理操作

卷积层：进行特征提取，对于图片中的每一个特征首先局部感知，然后更高层次对局部进行综合操作，从而得到全局信息，还有权值共享

池化层 ： 通过去掉Feature Map中不重要的样本，进一步减少参数数量。用于特征降维，压缩数据和参数的数量，减小过拟合，同时提高模型的容错性。

全连接层：对提取的特征进行分类，最后的全连接层就是把之前得到的特征用来分类或者回归

## 2.**卷积核有什么类型？各卷积作用？**
-   **一般卷积** ****实现跨通道的交互和信息整合;进行卷积核通道数的降维和升维
-   **扩张的卷积** ****使用3内核进行2D卷积，扩展率为2且无填充
-   **转置卷积** ****使卷积过程恢复
-   **可分离的卷积** ****执行空间卷积，同时保持通道分离，然后进行深度卷积

## 3.**卷积网络与前馈神经网络最显著的特点？**
卷积网络：包含卷积计算且具有[深度](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6&spm=1001.2101.3001.7020)结构的前馈神经网络。局部连接，权重共享以及子采样

前馈神经网络：每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。
## 4.**卷积层具有哪些参数？**
学习率的系数、卷积核的输出通道数、卷积核的大小、卷积核的步长、权重初始化方式、偏置项的初始化、分组、通道数
# 二.循环神经网络
## 1.**循环神经网络与卷积神经网络区别？**
卷积神经网络没有时序性的概念，输入直接和输出挂钩；循环神经网络具有时序性，当前决策跟前一次决策有关，具有短期记忆能力。

cnn多用于处理图像，rnn一般处理文字，音频等与时序相关的问题。
## 2.**循环神经网络具有哪些不同类型？**

序列到类别模式、同步的序列到序列模式、异步的序列到序列模式，一对多序列模式
## 3.**简述LSTM网络核心思想与各门的作用？**
LSTM 通过一种名为**门**的结构控制 cell 的状态，并向其中删减或增加信息。

遗忘门决定哪些信息需要从细胞状态中被遗忘。

输入门确定哪些新信息能够被存放到细胞状态中。

输出门用来确定下一个隐藏状态的值，隐藏状态包含了先前输入的信息。
## 4.**GRU与LSTM具有哪些不同？**
将遗忘门和输入门合并为一个门：更新门，此外另一门叫做重置门。

不引入额外的内部状态c，直接在当前状态ht和历史状态ht-1之间引入线性依赖关系。
## 5.**循环神经网络应用领域？**
**语音识别**：输入的语音数据，生成相应的语音文本信息。比如微信的语音转文字功能。

**机器翻译**：不同语言之间的相互转换。像有道翻译、腾讯翻译官等。最近微软据说实现了中翻英媲美人类的水平

**音乐生成**：使用RNN网络生成音乐，一般会用到RNN中的LSTM算法（该算法可以解决RNN网络中相距较远的节点梯度消失的问题）。

**文本生成**：利用RNN亦可以生成某种风格的文字。

**情感分类**：输入文本或者语音的评论数据，输出相应的打分数据。

**DNA** **序列分析**：输入的DNA序列，输出蛋白质表达的子序列。

**视频行为识别**：识别输入的视频帧序列中的人物行为。

**实体名字识别**：从文本中识别实体的名字。
# 三、自编码器
## 1.**自编码器最显著的特征？**
自编码器是一种无监督的数据维度压缩和数据特征表达方法。
## 2.**自编码器变种有哪些？**
-   普通的自编码器

-   多层自编码器

-   卷积自编码器

-   正则化的自编码器

-   稀疏自编码器

-   降噪自编码器

## 3.**自编码器应用领域？**
-   **数据去噪**

    **为进行可视化而降维**
